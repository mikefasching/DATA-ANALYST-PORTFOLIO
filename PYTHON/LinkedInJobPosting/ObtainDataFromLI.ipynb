{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d728870b-0a18-4cef-9d1a-82dde3fa235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33715aa9-45ff-497b-9222-06bfbe8b091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log in to LinkedIn to access data\n",
    "def login(driver):\n",
    "    url = \"https://www.linkedin.com/checkpoint/rm/sign-in-another-account?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin\"\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    driver.get(url)\n",
    "\n",
    "    username = driver.find_element_by_id(\"username\")\n",
    "    password = driver.find_element_by_id(\"password\")\n",
    "    username.send_keys(\"????\")    #USERNAME to LI\n",
    "    password.send_keys(\"????\")    #PASSWORD to LI\n",
    "\n",
    "def search(driver):\n",
    "    time.sleep(20)\n",
    "    driver.get(\"https://www.linkedin.com/jobs/search/?keywords=data%20scientist&location=New%20York%2C%20United%20States\")\n",
    "\n",
    "#grabs results fetched\n",
    "def get_n_results(driver):\n",
    "  time.sleep(2)\n",
    "  results_div = driver.find_element_by_xpath('//*[@id=\"main\"]/div/div[2]/div[1]/header/div[1]/small/div/span')\n",
    "  n_string = results_div.text\n",
    "  n = int(n_string.split()[0].replace(',',\"\"))\n",
    "  return n \n",
    "\n",
    "#Finds job ul div\n",
    "def get_jobs(driver):\n",
    "  ul_div = driver.find_element_by_xpath(\"//main[@id='main']//ul[1]\")\n",
    "  return ul_div\n",
    "    \n",
    "#Scrolls to properly load page\n",
    "def scroll_down(driver):\n",
    "    time.sleep(2)\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"main\"]/div/div[2]/div[1]/div')\n",
    "    scroll_height = element.get_property(\"scrollHeight\")\n",
    "\n",
    "    # Set the scroll step and delay between steps\n",
    "    scroll_step = 150\n",
    "    delay = 0.1\n",
    "\n",
    "    current_scroll = 0\n",
    "    while current_scroll < scroll_height:\n",
    "        driver.execute_script(\"arguments[0].scrollTo(0, arguments[1]);\", element, current_scroll)\n",
    "        time.sleep(delay)\n",
    "        current_scroll += scroll_step\n",
    "\n",
    "    # Wait for a short period for the new content to load\n",
    "    time.sleep(2)\n",
    "      \n",
    "def get_job_urls(jobs,driver,job_urls = {}):\n",
    "  i = 1\n",
    "  #Collects job urls,location role cand company \n",
    "  #the final result updates the input dictionary and appends a key value pair with the format\n",
    "  #    url:{'company':company,'location':location,'role':role}\n",
    "  while True: \n",
    "    try:\n",
    "        WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH,f\"//main[@id='main']//ul[1]\")))\n",
    "        url = jobs.find_element_by_xpath(f\"//main[@id='main']//ul[1]//li[{i}]//a[1]\").get_attribute(\"href\")\n",
    "        role = jobs.find_element_by_xpath(f\"//main[@id='main']//ul[1]//li[{i}]//a[1]\").text\n",
    "        company = jobs.find_element_by_xpath(f\"//main[@id='main']//ul[1]//li[{i}]//div[1]//div[1]//div[1]//div[2]//div[2]//span[1]\").text\n",
    "        location = driver.find_element_by_xpath(f\"//main[@id='main']//ul[1]//li[{i}]//li[1]\").text\n",
    "        job_urls.update({url:{'company':company,'location':location,'role':role}})\n",
    "        i+=1\n",
    "    except:\n",
    "        return job_urls\n",
    "\n",
    "def load_next_page(driver):\n",
    "  #loads next page for url retrival\n",
    "  curr= driver.find_element_by_xpath('//*[@aria-current=\"true\"]').text\n",
    "  next = driver.find_element_by_xpath(f'//*[@aria-label=\"Page {int(curr)+1}\"]')\n",
    "  next.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74f1130-3290-4913-bbfe-ef81faf13089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run theough the entire process of fetch in urls logining in and grabing job descriptions\n",
    "driver = webdriver.Chrome(r'C:\\Program Files\\Google\\Chrome\\Application\\chromedriver.exe')\n",
    "login(driver)\n",
    "search(driver)\n",
    "n = get_n_results(driver)\n",
    "if n > 350: #makes sure only 350 or less job openings are scraped (too much time otherwise)\n",
    "    n = 350\n",
    "pages = int(n/25)\n",
    "job_dict ={}\n",
    "#iterate through the amount of pages given\n",
    "for i in range(pages):\n",
    "  scroll_down(driver)\n",
    "  jobs = get_jobs(driver)\n",
    "  get_job_urls(jobs,driver,job_urls = job_dict)\n",
    "  load_next_page(driver)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83de3191-e077-436b-8008-8289dd691fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description(driver, job_dict):\n",
    "    fail = []\n",
    "    # Iterate through the url list to scrape the descriptions\n",
    "    for url in list(job_dict.keys()):\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            if driver.current_url != url:\n",
    "                print(f'failed at {url}')\n",
    "                # remove broken urls\n",
    "                job_dict.pop(url)\n",
    "            # scrape\n",
    "            applicants_str = driver.find_element_by_xpath(\"//*[self::span or self::strong][contains(text(), 'applicant') or contains(text(), 'applicants')]\").text\n",
    "            applicant_num = int(''.join(filter(str.isdigit, applicants_str)))\n",
    "            \n",
    "            post_dates = driver.find_elements_by_xpath(\"//*[contains(text(), 'ago')]\")\n",
    "            for post_date in post_dates:\n",
    "                text = post_date.text\n",
    "                post_date_days = re.search(r'(\\d+)\\s+(month|week|day|hour|minute|second)s?\\s+ago', text)\n",
    "                if post_date_days:\n",
    "                    value = int(post_date_days.group(1))\n",
    "                    unit = post_date_days.group(2)\n",
    "            \n",
    "                    if unit == \"month\":\n",
    "                        value *= 30\n",
    "                    elif unit == \"week\":\n",
    "                        value *= 7\n",
    "                    elif unit in [\"day\", \"hour\", \"minute\", \"second\"]:\n",
    "                        # Handle other units as needed\n",
    "                        pass\n",
    "                \n",
    "            description_element = driver.find_element_by_xpath(\"//h2[text()='About the job']/parent::div\")\n",
    "            description = description_element.get_attribute(\"textContent\")\n",
    "            job_dict.get(url).update({\"applicants\": applicant_num, \"description\": description, \"post_days\": value})\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {url}: {str(e)}\")\n",
    "            fail.append(url)\n",
    "    return job_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2575f10-64f9-4728-9917-5068a11636b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dict = get_description(driver,job_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b9b7a2f-8a84-4727-9ae1-74a24a3763e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dict to CSV\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.datetime.now()\n",
    "\n",
    "# Format the date and time as a string\n",
    "datetime_string = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Append the formatted date and time to the original filename\n",
    "filename = f\"job_data_scientist_NYC_{datetime_string}.csv\"# Specify the file path and filename \n",
    "\n",
    "with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header row\n",
    "    writer.writerow(['url', 'company', 'location', 'role', 'description', 'applicants','post_days'])\n",
    "    \n",
    "    # Write the data row\n",
    "    for url, data in job_dict.items():\n",
    "        row = [url] + [data.get(key, '') for key in ['company', 'location', 'role', 'description', 'applicants','post_days']]\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec072ed8-5e93-4f6e-922e-4c8a8b2d82d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
